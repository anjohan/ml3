\documentclass[11pt,british,a4paper]{article}
%\pdfobjcompresslevel=0
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage[includeheadfoot,margin=0.8 in]{geometry}
\usepackage{siunitx,physics,cancel,upgreek,varioref,listings,booktabs,pdfpages,ifthen,polynom,todonotes}
%\usepackage{minted}
\usepackage[backend=biber]{biblatex}
\DefineBibliographyStrings{english}{%
      bibliography = {References},
}
\addbibresource{sources.bib}
\usepackage{mathtools,upgreek,bigints}
\usepackage{babel}
\usepackage{graphicx}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tocloft}
\usepackage[T1]{fontenc}
%\usepackage{fouriernc}
% \usepackage[T1]{fontenc}
\usepackage{mathpazo}
% \usepackage{inconsolata}
%\usepackage{eulervm}
%\usepackage{cmbright}
%\usepackage{fontspec}
%\usepackage{unicode-math}
%\setmainfont{Tex Gyre Pagella}
%\setmathfont{Tex Gyre Pagella Math}
%\setmonofont{Tex Gyre Cursor}
%\renewcommand*\ttdefault{txtt}
\graphicspath{{figs/}}
\usepackage[scaled]{beramono}
\usepackage{fancyhdr}
\usepackage[utf8]{inputenc}
\usepackage{textcomp}
\usepackage{lastpage}
\usepackage{microtype}
\usepackage[font=normalsize]{subcaption}
\usepackage{luacode}
\usepackage[linktoc=all, bookmarks=true, pdfauthor={Anders Johansson},pdftitle={FYS-STK4155 Project 3}]{hyperref}
\usepackage{tikz,pgfplots,pgfplotstable}
\usepgfplotslibrary{colorbrewer}
\usepgfplotslibrary{external}
\tikzset{external/system call={lualatex \tikzexternalcheckshellescape -halt-on-error -interaction=batchmode -jobname "\image" "\texsource"}}
\tikzexternalize%[prefix=tmp/, mode=list and make]
\pgfplotsset{cycle list/Dark2}
\pgfplotsset{compat=1.8}
\pgfmathsetseed{12}
\renewcommand{\CancelColor}{\color{red}}
\let\oldexp=\exp
\renewcommand{\exp}[1]{\mathrm{e}^{#1}}


\labelformat{section}{#1}
\labelformat{subsection}{section~#1}
\labelformat{subsubsection}{paragraph~#1}
\labelformat{equation}{equation~(#1)}
\labelformat{figure}{figure~#1}
\labelformat{table}{table~#1}

\renewcommand{\footrulewidth}{\headrulewidth}

%\setcounter{secnumdepth}{4}
\setlength{\parindent}{0cm}
\setlength{\parskip}{1em}

\definecolor{bluekeywords}{rgb}{0.13,0.13,1}
\definecolor{greencomments}{rgb}{0,0.5,0}
\definecolor{redstrings}{rgb}{0.9,0,0}
\lstset{rangeprefix=!/,
    rangesuffix=/!,
    includerangemarker=false}
\lstset{showstringspaces=false,
    basicstyle=\small\ttfamily,
    keywordstyle=\color{bluekeywords},
    commentstyle=\color{greencomments},
    numberstyle=\color{bluekeywords},
    stringstyle=\color{redstrings},
    breaklines=true,
    %texcl=true,
    language=Fortran,
    morekeywords={norm2,class,deferred}
}
\colorlet{DarkGrey}{white!20!black}
\newcommand{\eqtag}[1]{\refstepcounter{equation}\tag{\theequation}\label{#1}}
\hypersetup{hidelinks=True}

\sisetup{detect-all}
\sisetup{exponent-product = \cdot, output-product = \cdot,per-mode=symbol}
% \sisetup{output-decimal-marker={,}}
\sisetup{round-mode = off, round-precision=3}
\sisetup{number-unit-product = \ }

\allowdisplaybreaks[4]
\fancyhf{}

\rhead{Project 3}
\rfoot{Page~\thepage{} of~\pageref{LastPage}}
\lhead{FYS-STK4155}

%\definecolor{gronn}{rgb}{0.29, 0.33, 0.13}
\definecolor{gronn}{rgb}{0, 0.5, 0}

\newcommand{\husk}[2]{\tikz[baseline,remember picture,inner sep=0pt,outer sep=0pt]{\node[anchor=base] (#1) {\(#2\)};}}
\newcommand{\artanh}[1]{\operatorname{artanh}{\qty(#1)}}
\newcommand{\matrise}[1]{\begin{pmatrix}#1\end{pmatrix}}
\DeclareMathOperator{\Proj}{Proj}
\DeclareMathOperator{\Col}{Col}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\MSE}{MSE}

\newread\infile

\setcounter{tocdepth}{2}
\numberwithin{equation}{section}

\pgfplotstableset{
    every head row/.style={before row=\toprule,after row=\midrule},
    every last row/.style={after row=\bottomrule}
}

\pgfplotsset{
    tick align=outside,
    tick pos=left,
    xmajorgrids,
    x grid style={white},
    ymajorgrids,
    y grid style={white},
    axis line style={white},
    axis background/.style={fill=white!89.803921568627459!black}
}

%start
\begin{document}
\tikzexternaldisable
\title{FYS-STK4155: Project 3}
\author{Anders Johansson}
%\maketitle

\begin{titlepage}
%\includegraphics[width=\textwidth]{fysisk.pdf}
\vspace*{\fill}
\begin{center}
\textsf{
    \Huge \textbf{Project 3}\\\vspace{0.5cm}
    \Large \textbf{FYS-STK4155 --- Applied data analysis and machine learning}\\
    \vspace{8cm}
    Anders Johansson\\
    \today\\
}
\vspace{1.5cm}
\includegraphics{uio.pdf}\\
\vspace*{\fill}
\end{center}
\end{titlepage}
\null
\pagestyle{empty}
\newpage

\pagestyle{fancy}
\setcounter{page}{1}

\begin{abstract}
    This report deals with the numerical solution of the one-dimensional diffusion equation. The traditional Forward Euler scheme is compared with the novel approach of using neural networks. Tensorflow is used for the neural network due to its ability to handle cost functions defined in terms of the derivatives of the neural network's output with respect to its input. While Forward Euler performed better both in terms of accuracy and runtime, the neural networks also showed promising results. A combination of the hyperbolic tangent as activation function and the Adam optimiser performed best, and the number of nodes per layer proved more important than the number of layers, as shown by the three best architectures being \((100,100,100)\), \((100,100)\) and \((10,10,10,10,10)\) nodes in the hidden layers.
\end{abstract}
\pagestyle{fancy}
\thispagestyle{fancy}
\tableofcontents
\pagestyle{fancy}
\thispagestyle{fancy}

%  _       _                 _            _   _
% (_)_ __ | |_ _ __ ___   __| |_   _  ___| |_(_) ___  _ __
% | | '_ \| __| '__/ _ \ / _` | | | |/ __| __| |/ _ \| '_ \
% | | | | | |_| | | (_) | (_| | |_| | (__| |_| | (_) | | | |
% |_|_| |_|\__|_|  \___/ \__,_|\__,_|\___|\__|_|\___/|_| |_|
\section{Introduction}
Differential equations are an important tool in physics for understanding the world around us. Classical motion is determined by Newton's laws of motion, which are ordinary differential equations, while relativistic motion and quantum mechanics are summarised in partial differential equations. Unfortunately, differential equations are hard to solve analytically, and numerical techniques are therefore employed. Historically, finite difference and finite element schemes have been used with great success. This report utilises a new approach, namely artificial neural networks, and compares it with a simple finite difference scheme.

Artificial neural networks are used to solve the diffusion equation, which models such important phenomena as heat dissipation and diffusion. Tensorflow, Google's leading framework for machine learning and artificial intelligence, is used as a high-level, intuitive, ``black box'' approach. The results are compared to the simplest finite difference scheme, namely Forward Euler.

Since the main theory of neural networks was derived in an earlier project, this report mainly revolves around their application to neural networks. First, however, the report starts off with a derivation of the finite difference scheme with which the neural network is compared, before going through the details of how to use neural networks for solving differential equations. Both methods are then applied to the diffusion equation and their results are compared.

%  _   _                                             _
% | |_| |__   ___  ___  _ __ _   _    __ _ _ __   __| |
% | __| '_ \ / _ \/ _ \| '__| | | |  / _` | '_ \ / _` |
% | |_| | | |  __/ (_) | |  | |_| | | (_| | | | | (_| |
%  \__|_| |_|\___|\___/|_|   \__, |  \__,_|_| |_|\__,_|
%                 _   _      |___/    _
%  _ __ ___   ___| |_| |__   ___   __| |___
% | '_ ` _ \ / _ \ __| '_ \ / _ \ / _` / __|
% | | | | | |  __/ |_| | | | (_) | (_| \__ \
% |_| |_| |_|\___|\__|_| |_|\___/ \__,_|___/
\section{Theory and methods}
\subsection{The diffusion equation}
The standard diffusion equation is
\begin{equation}
    \pdv{u}{t} = \nabla^2 u, \quad \text{or, in one dimension,}\quad \pdv{u}{t} = \pdv[2]{u}{x},
\end{equation}
for \(x\in\qty[0,1]\) and \(t\geq0\). In this project, the initial and boundary conditions are
\begin{equation}
    u(x,0) = \sin(\pi x), \qquad u(0,t) = u(1,t) = 0.
\end{equation}
The analytical solution is obviously
\begin{equation}
    u(x,t) = \exp{-\pi^2 t} \sin(\pi x).
\end{equation}


\subsection{Finite difference methods}
Differential equations are equations where the unknown is a function, and the equation consists of terms containing the function and its derivatives. Finite difference methods simply replace the derivatives with numerical approximations based on Taylor expansions, and solve for the unknown function values.

\subsubsection{The Forward Euler scheme}
The simplest approximations to numerical derivatives which can be inserted in the diffusion equation are
\begin{equation}
    \pdv{u(x,t)}{t} \approx \frac{u(x,t+\Delta t) - u(x,t)}{\Delta t}, \qquad
    \pdv[2]{u(x,t)}{x} \approx \frac{u(x+\Delta x, t) - 2u(x,t) + u(x-\Delta x,t)}{\Delta x^2}.
\end{equation}
Discretising with \(x_i=i\Delta x\) for \(i=0,\ldots,N_x\) with \(\Delta x=1/N_x\), \(t^j=j\Delta t\) for \(j=0,\ldots,N_t\) with \(\Delta t=T/N_t\) and \(u_i^j = u(x_i,t^j)\), the diffusion equation with these approximations is
\begin{equation}
    \frac{u_i^{j+1} - u_i^j}{\Delta t} = \frac{u_{i+1}^j - 2u_i^j + u_{i-1}^j}{\Delta x^2}
\end{equation}
with boundary conditions
\begin{equation}
    u_i^0 = \sin(i\pi\Delta x),\quad
    u_0^j = u_{N_x}^j = 0.
\end{equation}
Since \(\qty{u_i^0}\) is known, the function values at the subsequent timesteps can be found recursively by
\begin{equation}
    u_i^{j+1} = u_i^j + \frac{\Delta t}{\Delta x^2} \qty(u_{i+1}^j - 2u_i^j + u_{i-1}^j)
\end{equation}
for \(i=1,\ldots,N_x-1\). The error terms are proportional to \(\Delta t\) and \(\Delta x^2\) due to a first order approximation to the temporal derivative and a second order approximation to the spatial derivative. Stability is ensured\cite{tveito} when
\begin{equation}
    \frac{\Delta t}{\Delta x^2} \leq \frac{1}{2},
\end{equation}
so \(\Delta t = \tfrac{1}{2}\Delta x^2\) is used.

\subsection{Neural networks for differential equations}
A neural network can approximate any function, and it should therefore also be able to approximate the solution of a differential equation such as the diffusion equation. The neural network approximates by minimising a cost function, and the challenge is therefore to find a suitable cost function.

The simplest approach is simply to use the square of the difference between the two sides of the diffusion equation as the cost function,
\begin{equation}
    Q = \qty(\pdv{u}{t} - \pdv[2]{u}{x})^2.
\end{equation}
It seems difficult to differentiate this cost function with respect to the weights and biases of the neural network. I have therefore chosen to use Tensorflow\cite{tensorflow} for this project, since it contains functionality for calculating automatic derivatives (something my otherwise brilliant Fortran network lacks).

Another difficulty is to fulfill the initial and boundary conditions. One approach may be to add terms to the cost function which penalise approximations which do not fulfill the initial and boundary conditions. Another approach, which is found more often in literature\cite{nnde} and therefore employed in this report, is to not use the output of the neural network directly, but rather combining a neural network with other functions in such a way that the initial and boundary conditions are guaranteed to be fulfilled.

In the case of the diffusion equation with our set of conditions, a natural choice is
\begin{equation}
    u(x,t) = \sin(\pi x) + tx\qty(1-x)n(x,t),
\end{equation}
where \(n(x,t)\) is the output from the neural network.

%                      _ _
%  _ __ ___  ___ _   _| | |_ ___
% | '__/ _ \/ __| | | | | __/ __|
% | | |  __/\__ \ |_| | | |_\__ \
% |_|  \___||___/\__,_|_|\__|___/
\section{Results}
\subsection{Forward Euler}
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                thick,
                width=\textwidth,
                height=2.5in,
                xlabel={\(x\)},
                ylabel={\(u(x,t)\)},
                legend style={draw=none,fill=none,at={(-0.01,1.03)}, anchor=north west},
                legend cell align=left,
                samples=100
            ]
                \addplot+[mark=none, gray, domain=0:1] {exp(-pi*pi*0.2)*sin(deg(pi*x))};
                \addlegendentry{\(\exp{-\pi^2 t}\sin(\pi x)\)};
                \addplot+[mark=none] table[y index=1] {data/euler_2.0E-01.dat};
                \addlegendentry{\(t=\num{0.0}\)};
                \addplot+[mark=none] table[y index=2] {data/euler_2.0E-01.dat};
                \addlegendentry{\(t=\num{0.1}\)};
                \addplot+[mark=none] table[y index=3] {data/euler_2.0E-01.dat};
                \addlegendentry{\(t=\num{0.2}\)};
                \addplot+[mark=none, gray, domain=0:1] {sin(deg(pi*x))};
                \addplot+[mark=none, gray, domain=0:1] {exp(-pi*pi*0.1)*sin(deg(pi*x))};
            \end{axis}
        \end{tikzpicture}
        \caption{\(\Delta x=\num{0.2}\)}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                thick,
                width=\textwidth,
                height=2.5in,
                xlabel={\(x\)},
                ylabel={\(u(x,t)\)},
                legend style={draw=none,fill=none,at={(0.98,0.98)}, anchor=north east},
                legend cell align=left,
            ]
                \addplot+[mark=none] table[y index=1] {data/euler_1.0E-02.dat};
                \addlegendentry{\(t=\num{0.0}\)};
                \addplot+[mark=none] table[y index=2] {data/euler_1.0E-02.dat};
                \addlegendentry{\(t=\num{0.1}\)};
                \addplot+[mark=none] table[y index=3] {data/euler_1.0E-02.dat};
                \addlegendentry{\(t=\num{0.2}\)};
            \end{axis}
        \end{tikzpicture}
        \caption{\(\Delta x=\num{0.01}\)}
    \end{subfigure}
    \caption{Results from solving the diffusion equation with the Forward Euler finite difference scheme for two different values of \(\Delta x\) and \(\Delta t=\frac{1}{2}\Delta x^2\). While the solution for \(\Delta x=\num{0.2}\) deviates visibly from the analytic solution, \(\Delta x=\num{0.01}\) gives a good approximation.}
\end{figure}

\subsection{Neural networks}
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                thick,
                width=\textwidth,
                height=2.5in,
                xlabel={\(x\)},
                ylabel={\(u(x,t)\)},
                legend style={draw=none,fill=none,at={(-0.01,1.03)}, anchor=north west},
                legend cell align=left,
                samples=100
            ]
                \addlegendentry{\(\exp{-\pi^2 t}\sin(\pi x)\)};
                \addplot+[mark=none] table[y index=1] {data/nn_u_100.dat};
                \addlegendentry{\(t=\num{0.0}\)};
                \addplot+[mark=none] table[y index=2] {data/nn_u_100.dat};
                \addlegendentry{\(t=\num{0.1}\)};
                \addplot+[mark=none] table[y index=3] {data/nn_u_100.dat};
                \addlegendentry{\(t=\num{0.2}\)};
            \end{axis}
        \end{tikzpicture}
        \caption{\(\num{100}\) epochs}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                thick,
                width=\textwidth,
                height=2.5in,
                xlabel={\(x\)},
                ylabel={\(u(x,t)\)},
                legend style={draw=none,fill=none,at={(0.98,0.98)}, anchor=north east},
                legend cell align=left,
            ]
                \addplot+[mark=none] table[y index=1] {data/nn_u_10000.dat};
                \addlegendentry{\(t=\num{0.0}\)};
                \addplot+[mark=none] table[y index=2] {data/nn_u_10000.dat};
                \addlegendentry{\(t=\num{0.1}\)};
                \addplot+[mark=none] table[y index=3] {data/nn_u_10000.dat};
                \addlegendentry{\(t=\num{0.2}\)};
            \end{axis}
        \end{tikzpicture}
        \caption{\(\num{10000}\) epochs}
    \end{subfigure}
    \caption{Approximation to the solution of the diffusion equation provided by a neural network with \(6\) hidden layers with \(10\) nodes each. While the network gives horrible results after \(10\) epochs, the solution is seen to approach the analytic solution. As seen in~\vref{fig:nncost}, however, it seems that the minimisation may have found a local minimum.}
\end{figure}
\begin{figure}
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            thick,
            width=5in,
            height=3in,
            xmode=log,
            ymode=log,
            xlabel={Epochs},
            ylabel={Error},
            legend style={draw=none, fill=none, at={(0.98,0.98)}, anchor=north east},
            legend cell align=left
        ]
            \addplot+[mark=none] table[y index=1] {data/nn_cost.dat};
            \addlegendentry{\(\frac{1}{N_xN_t}\norm{\pdv{u}{t}-\pdv[2]{u}{x}}_2^2\)};
            \addplot+[mark=none] table[y index=2] {data/nn_cost.dat};
            \addlegendentry{\(\frac{1}{N_xN_t}\norm{u - u_\text{exact}}_2\)};
        \end{axis}
    \end{tikzpicture}
    \caption{The error in the neural network's approximation, both in terms of the cost function defined as the squared difference between the two sides of the diffusion equation and the deviation from the known, exact solution. While the cost function is eventually reduced, the graphs indicate that the minimiser may be stuck in a local minimum. It is therefore clearly advantageous to do a parameter search and try different architectures and minimisers.}\label{fig:nncost}
\end{figure}

\subsubsection{Parameter sweep}
Simulations similar to the ones above were run for a variety of combinations of activation functions, optimisers and network architectures. The best results (defined as having the smallest cost) are shown below.
\begin{table}[H]
    \centering
    \caption{Best choices of parameters, defined as giving the smallest cost function after \(\num{10000}\) epochs (``Error'' denotes the deviation from the analytic solution). Approximately \(\num{1000}\) parameter combinations were tested. Adam denotes the \lstinline{tf.train.AdamOptimizer} with default parameters, which uses an adaptive learning rate \(\alpha\) and momentum \(\gamma\), thereby outperforming batch gradient descent with fixed learning rate and momentum. The architecture gives the numbers of nodes in the hidden layers. More results are available at \href{https://github.com/anjohan/ml3/tree/master/data}{github.com/anjohan/ml3/data}.}
    \pgfplotstabletypeset[
            columns/{Activation function}/.style={string type},
            columns/{Optimiser}/.style={string type},
            columns/{Architecture}/.style={string type},
            fixed zerofill, precision=1
        ]
        {data/nn_cost_table_small.dat}
\end{table}
%                                                                   _
%  ___ _   _ _ __ ___  _ __ ___   __ _ _ __ _   _    __ _ _ __   __| |
% / __| | | | '_ ` _ \| '_ ` _ \ / _` | '__| | | |  / _` | '_ \ / _` |
% \__ \ |_| | | | | | | | | | | | (_| | |  | |_| | | (_| | | | | (_| |
% |___/\__,_|_| |_| |_|_| |_| |_|\__,_|_|   \__, |  \__,_|_| |_|\__,_|
%                       _           _       |___/
%   ___ ___  _ __   ___| |_   _ ___(_) ___  _ __
%  / __/ _ \| '_ \ / __| | | | / __| |/ _ \| '_ \
% | (_| (_) | | | | (__| | |_| \__ \ | (_) | | | |
%  \___\___/|_| |_|\___|_|\__,_|___/_|\___/|_| |_|
\section{Summary and conclusion}
The one-dimensional diffusion equation was solved with both the traditional Forward Euler scheme and the more novel approach of using neural networks. While the neural networks converged towards the analytic solution, they were clearly beaten by Euler's method, which performed better in terms of both of accuracy and runtime. The main conclusion is, as in the previous project, to use specialised methods whenever possible, while neural networks can offer an acceptable solution which does not require much knowledge from the user --- especially when using a library as powerful as Tensorflow.

A parameter sweep was performed in order to find the optimal solution of activation functions, optimisers and network architectures. Results showed that networks with fewer layers with more nodes performed better than deeper networks with fewer nodes per layer. The networks performed best when using the hyperbolic tangent as activation function and the Adam optimiser, with its adaptive learning rate and momentum.

A simple extension of this project would be to try more parameter combinations, and deeper neural networks or networks with more nodes per layer. Performance issues will be mitigated if these simulations are run on a GPU.\ More substantial extensions include for example other network types, as the approximation of \(u(x,t)\) is similar to an image problem and may therefore benefit from a convolutional neural network.

%                                   _ _
%   __ _ _ __  _ __   ___ _ __   __| (_)_  __
%  / _` | '_ \| '_ \ / _ \ '_ \ / _` | \ \/ /
% | (_| | |_) | |_) |  __/ | | | (_| | |>  <
%  \__,_| .__/| .__/ \___|_| |_|\__,_|_/_/\_\
%       |_|   |_|
%\clearpage
%\appendix
%\part*{Appendix}
%\addcontentsline{toc}{part}{Appendix}
%\renewcommand{\thesection}{\Alph{section}}
%\labelformat{section}{appendix~#1}
%\labelformat{subsection}{appendix~#1}


\clearpage
\nocite{*}
\printbibliography{}
\addcontentsline{toc}{section}{\bibname}
\end{document}
